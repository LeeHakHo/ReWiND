# @package _global_

defaults:
  - _self_

# Because RLPD is slower, we will log more frequently
logging:
  eval_freq: 0
  video_freq: 0

general_training:
  algo: rlpd
  name: rlpd
  action_noise: 0.0 # sigma
  learning_rate: 1e-3

  # offline rl parameters
  n_critics: 10
  n_critics_to_sample: 2

  # RLPD-specific parameters
  rlpd_train_critic_with_entropy: true
  # Note: RLPD's offline step uses default params defined in base_config.yaml
  rlpd_offline_algo: iql

model:
  policy_layer_norm: true
  critic_layer_norm: true
  pi_net_arch: [1024, 768, 512]
  qf_net_arch: [1024, 1024, 512]

offline_training:
  offline_training_steps: 0
  ckpt_path: null  # Path to load checkpoint from, if provided will skip offline training
  critic_update_ratio: 1

online_training:
  total_time_steps: 100000
  mix_buffers_ratio: 0.5
  warm_start_online_rl: false
  learning_starts: 50
  critic_update_ratio: 4
  gradient_steps: 1